# example.conf: A single-node Flume configuration

# Name the components on this agent
# first-agent in below line is the agent name.
# You can name your agent whatever you want
mp-agent.sources = logs_source
mp-agent.sinks = hdfs_snk spark_sink
mp-agent.channels = mem_channel spark_mem_channel

# Describe/configure the source
mp-agent.sources.logs_source.type = exec
mp-agent.sources.logs_source.command = tail -f /user/beercafeguy/code/scripts/gen_logs/logs/access.log

# Describe the sink
# Sink for HDFS
mp-agent.sinks.hdfs_snk.type = hdfs
mp-agent.sinks.hdfs_snk.hdfs.path = /user/beercafeguy/data/web_logs/
mp-agent.sinks.hdfs_snk.hdfs.filePrefix = apache
mp-agent.sinks.hdfs_snk.hdfs.fileSuffix = .log
mp-agent.sinks.hdfs_snk.hdfs.rollSize = 2048

# Sink for Spark Streaming (push mode)
# mp-agent.sinks.spark_sink.type = avro
# mp-agent.sinks.spark_sink.hostname = nn.host.beercafeguy.com
# mp-agent.sinks.spark_sink.port = 9760

# Sink for Spark Streaming (pull mode)
mp-agent.sinks.spark_sink.type = org.apache.spark.streaming.flume.sink.SparkSink
mp-agent.sinks.spark_sink.hostname = nn.host.beercafeguy.com
mp-agent.sinks.spark_sink.port = 9760
 

# Use a channel which buffers events in memory
# Channel for HDFS
mp-agent.channels.mem_channel.type = memory
mp-agent.channels.mem_channel.capacity = 1000
mp-agent.channels.mem_channel.transactionCapacity = 100

# channel for spark
mp-agent.channels.spark_mem_channel.type = memory
mp-agent.channels.spark_mem_channel.capacity = 1000
mp-agent.channels.spark_mem_channel.transactionCapacity = 100

# Bind the source and sink to the channel
mp-agent.sources.logs_source.channels = mem_channel spark_mem_channel
mp-agent.sinks.hdfs_snk.channel = mem_channel
mp-agent.sinks.spark_sink.channel = spark_mem_channel